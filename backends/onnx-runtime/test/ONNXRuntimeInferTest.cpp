#include <gtest/gtest.h>
#include "ORTInfer.hpp"
#include <glog/logging.h>
#include <opencv2/opencv.hpp>
#include <fstream>
#include <iostream>
#include <filesystem>
#include <memory>

namespace fs = std::filesystem;

// Mock inference implementation for unit testing
class MockORTInfer {
public:
    MockORTInfer() = default;
    
    std::tuple<std::vector<std::vector<TensorElement>>, std::vector<std::vector<int64_t>>> 
    get_infer_results(const cv::Mat& input) {
        // Mock output: 1x1000 classification results
        std::vector<TensorElement> output_vector(1000);
        for (int i = 0; i < 1000; ++i) {
            output_vector[i] = static_cast<float>(i * 0.001f); // Mock probabilities
        }
        
        std::vector<std::vector<TensorElement>> output_vectors = {output_vector};
        std::vector<std::vector<int64_t>> shape_vectors = {{1, 1000}};
        
        return std::make_tuple(output_vectors, shape_vectors);
    }
    
    ModelInfo get_model_info() {
        ModelInfo info;
        info.addInput("input", {1, 3, 224, 224}, 1);
        info.addOutput("output", {1, 1000}, 1);
        return info;
    }
};

// Test fixture for ONNX Runtime backend
class ONNXRuntimeInferTest : public ::testing::Test {
protected:
    std::string model_path;
    bool has_real_model;
    std::unique_ptr<ORTInfer> real_infer;
    std::unique_ptr<MockORTInfer> mock_infer;

    void SetUp() override {
        has_real_model = false;
        model_path = "";
        
        // Check if model_path.txt exists (set by test script)
        std::ifstream modelPathFile("model_path.txt");
        if (modelPathFile) {
            std::getline(modelPathFile, model_path);
            if (!model_path.empty() && fs::exists(model_path)) {
                has_real_model = true;
                try {
                    real_infer = std::make_unique<ORTInfer>(model_path, false);
                    std::cout << "Using real model: " << model_path << std::endl;
                } catch (const std::exception& e) {
                    std::cout << "Failed to load real model, falling back to mock: " << e.what() << std::endl;
                    has_real_model = false;
                }
            }
        }
        
        if (!has_real_model) {
            mock_infer = std::make_unique<MockORTInfer>();
            std::cout << "Using mock inference for testing" << std::endl;
        }
    }
};

// Test basic functionality - works with both real model and mock
TEST_F(ONNXRuntimeInferTest, BasicInference) {
    cv::Mat input = cv::Mat::zeros(224, 224, CV_32FC3); // ResNet-18 expects 224x224 input
    cv::Mat blob;
    cv::dnn::blobFromImage(input, blob, 1.f / 255.f, cv::Size(224, 224), cv::Scalar(), true, false);
    
    std::vector<std::vector<TensorElement>> output_vectors;
    std::vector<std::vector<int64_t>> shape_vectors;
    
    if (has_real_model) {
        auto result = real_infer->get_infer_results(blob);
        output_vectors = std::get<0>(result);
        shape_vectors = std::get<1>(result);
    } else {
        auto result = mock_infer->get_infer_results(blob);
        output_vectors = std::get<0>(result);
        shape_vectors = std::get<1>(result);
    }

    ASSERT_FALSE(output_vectors.empty());
    ASSERT_FALSE(shape_vectors.empty());

    ASSERT_EQ(shape_vectors[0].size(), 2);
    ASSERT_EQ(shape_vectors[0][0], 1);
    ASSERT_EQ(shape_vectors[0][1], 1000);

    // Type checking
    ASSERT_TRUE(std::holds_alternative<float>(output_vectors[0][0]));
    
    // Value access checking
    ASSERT_NO_THROW({
        float value = std::get<float>(output_vectors[0][0]);
    });
    
    // Size checking
    ASSERT_EQ(output_vectors[0].size(), static_cast<size_t>(shape_vectors[0][1]));
    
    // Check all elements are of the expected type
    ASSERT_TRUE(std::all_of(output_vectors[0].begin(), output_vectors[0].end(), 
        [](const TensorElement& element) {
            return std::holds_alternative<float>(element);
        }));
}

// Integration test - only runs with real model
TEST_F(ONNXRuntimeInferTest, IntegrationTest) {
    if (!has_real_model) {
        GTEST_SKIP() << "Skipping integration test - no real model available";
    }
    
    // Test with real model
    cv::Mat input = cv::Mat::zeros(224, 224, CV_32FC3);
    cv::Mat blob;
    cv::dnn::blobFromImage(input, blob, 1.f / 255.f, cv::Size(224, 224), cv::Scalar(), true, false);
    
    auto [output_vectors, shape_vectors] = real_infer->get_infer_results(blob);
    
    // Verify real model produces reasonable results
    ASSERT_FALSE(output_vectors.empty());
    ASSERT_EQ(output_vectors[0].size(), 1000); // ImageNet classes
    
    // Check that output values are in reasonable range for probabilities/logits
    for (const auto& element : output_vectors[0]) {
        float value = std::get<float>(element);
        ASSERT_TRUE(std::isfinite(value)) << "Output contains non-finite value";
    }
    
    // Test model info retrieval
    auto model_info = real_infer->get_model_info();
    ASSERT_FALSE(model_info.getInputs().empty());
    ASSERT_FALSE(model_info.getOutputs().empty());
}

// Unit test - only runs with mock
TEST_F(ONNXRuntimeInferTest, MockUnitTest) {
    if (has_real_model) {
        GTEST_SKIP() << "Skipping mock unit test - real model is available";
    }
    
    cv::Mat input = cv::Mat::zeros(224, 224, CV_32FC3);
    auto [output_vectors, shape_vectors] = mock_infer->get_infer_results(input);
    
    // Test mock-specific behavior
    ASSERT_EQ(output_vectors[0].size(), 1000);
    
    // Verify mock data pattern
    for (int i = 0; i < 10; ++i) {
        float expected = i * 0.001f;
        float actual = std::get<float>(output_vectors[0][i]);
        ASSERT_FLOAT_EQ(expected, actual);
    }
    
    // Test model info from mock
    auto model_info = mock_infer->get_model_info();
    ASSERT_FALSE(model_info.getInputs().empty());
    ASSERT_FALSE(model_info.getOutputs().empty());
}

// GPU test - only runs if has_real_model and GPU available
TEST_F(ONNXRuntimeInferTest, GPUTest) {
    if (!has_real_model) {
        GTEST_SKIP() << "Skipping GPU test - no real model available";
    }
    
    try {
        // Try to create a GPU inference engine
        auto gpu_infer = std::make_unique<ORTInfer>(model_path, true);
        
        // If we got here, GPU is available, test inference
        cv::Mat input = cv::Mat::zeros(224, 224, CV_32FC3);
        cv::Mat blob;
        cv::dnn::blobFromImage(input, blob, 1.f / 255.f, cv::Size(224, 224), cv::Scalar(), true, false);
        
        auto [output_vectors, shape_vectors] = gpu_infer->get_infer_results(blob);
        
        // Basic validation
        ASSERT_FALSE(output_vectors.empty());
        ASSERT_EQ(output_vectors[0].size(), 1000);
    } catch (const std::exception& e) {
        // GPU not available or error occurred
        GTEST_SKIP() << "Skipping GPU test - GPU not available or error: " << e.what();
    }
}

// Test with different batch sizes - only runs with real model
TEST_F(ONNXRuntimeInferTest, BatchSizeHandling) {
    if (!has_real_model) {
        GTEST_SKIP() << "Skipping batch size test - no real model available";
    }
    
    try {
        size_t batch_size = 2;
        std::vector<std::vector<int64_t>> input_sizes = {{3, 224, 224}};
        
        auto batch_infer = std::make_unique<ORTInfer>(model_path, false, batch_size, input_sizes);
        auto model_info = batch_infer->get_model_info();
        
        ASSERT_FALSE(model_info.getInputs().empty());
        
        // Check that batch size is properly set in input tensor
        auto inputs = model_info.getInputs();
        ASSERT_EQ(inputs[0].batch_size, batch_size);
    } catch (const std::exception& e) {
        GTEST_SKIP() << "Batch size test failed: " << e.what();
    }
}

int main(int argc, char **argv) {
    ::testing::InitGoogleTest(&argc, argv);
    return RUN_ALL_TESTS();
}
